    <section>
        <br>
        <div class="container">
            <hr>
            <h4 id="2022">2022</h4>
            <hr>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="50%"
                         src="https://user-images.githubusercontent.com/22636930/250826756-42a2da46-a9e9-41e6-83e8-c01f7bca38ba.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">ARTFM: Augmented Reality Visualization of Tool Functionality Manuals in Operating Rooms. (Poster)</span>
                    </a>
                    <br>
                    <span style="font-weight: lighter">Kleinbeck, C.,</span> <span>Schieber, H.</span><span
                        style="font-weight: lighter">, Andress, S., Krautz, C., & Roth, D.
                </span><br>
                    <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                        (pp. 736-737). IEEE.</em>
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseARTFM" role="button"
                           aria-expanded="false" aria-controls="collapseARTFM">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/abstract/document/9757491">IEEE</a>
                    </p>
                    <div class="collapse" id="collapseARTFM">
                        <div class="card card-body">
                            <p>
                                Error-free surgical procedures are crucial for a patient's health. However, with the
                                increasing complexity and variety of surgical instruments, it is difficult for clinical
                                staff to acquire detailed assembly and usage knowledge leading to errors in process and
                                preparation steps. Yet, the gold standard in retrieving necessary information when
                                problems
                                occur is to get the paperbased manual. Reading through the necessary instructions is
                                time-consuming and decreases care quality. We propose ARTFM, a process integrated
                                manual,
                                highlighting the correct parts needed, their location, and step-by-step instructions to
                                combine the instrument using an augmented reality head-mounted display.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="50%"
                         src="https://user-images.githubusercontent.com/22636930/250825228-2340df9b-920a-4378-8fa1-956ba771fd79.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">A Mixed Reality Guidance System for Blind and Visually Impaired People (Poster)</span>

                    <br>
                    <span>Schieber, H.</span><span style="font-weight: lighter">, Kleinbeck, C., Pradel, C., Theelke, L., & Roth, D.</span>
                    <br>
                    <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                        (pp. 726-727). IEEE.</em>, 2022
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseBlind" role="button"
                           aria-expanded="false" aria-controls="collapseBlind">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/abstract/document/9757681">[IEEE]</a>

                    </p>
                    <div class="collapse" id="collapseBlind">
                        <div class="card card-body">
                            <p>
                                Persons affected by blindness or visual impairments are challenged by spatially
                                understanding unfamiliar environments. To obtain such understanding, they have to sense
                                their environment closely and carefully. Especially objects outside the sensing area of
                                analog assistive devices, such as a white cane, are simply not perceived and can be the
                                cause of collisions. This project proposes a mixed reality guidance system that aims at
                                preventing such problems. We use object detection and the 3D sensing capabilities of a
                                mixed reality head mounted device to inform users about their spatial surroundings.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://user-images.githubusercontent.com/22636930/170203786-c1c6de02-5314-4275-bd36-ae655670f4b5.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation</span>

                    <br>
                    <span>H. Schieber*</span>, <span style="font-weight: lighter">F. Duerr*, T. Schoen and J. Beyerer, *denotes equal contribution</span>
                    <br>
                    <em>Intelligent Vehicles Symposium (IV), Aachen, Germany</em>, 2022
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseIV" role="button"
                           aria-expanded="false" aria-controls="collapseIV">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/document/9827113">IEEE</a>
                        <a class="btn btn-dark"
                           href="https://arxiv.org/pdf/2205.13629.pdf">Arxiv</a>
                        <a class="btn btn-dark" href="https://hannahhaensen.github.io/pyfu/">Website</a>
                    </p>
                    <div class="collapse" id="collapseIV">
                        <div class="card card-body">
                            <p>
                                Robust environment perception for autonomous vehicles is a tremendous challenge, which
                                makes a diverse sensor set with e.g. camera, lidar and radar crucial. In the process of
                                understanding the recorded sensor data, 3D semantic segmentation plays an important
                                role. Therefore,
                                this work presents a pyramid-based deep fusion architecture for lidar and camera to
                                improve 3D
                                semantic segmentation of traffic scenes. Individual sensor backbones extract feature
                                maps of
                                camera images and lidar point clouds. A novel Pyramid Fusion Backbone fuses these
                                feature maps
                                at different scales and combines the multimodal features in a feature pyramid to compute
                                valuable multimodal, multi-scale features. The Pyramid Fusion Head aggregates these
                                pyramid
                                features and further refines them in a late fusion step, incorporating the final
                                features of the
                                sensor backbones. The approach is evaluated on two challenging outdoor datasets and
                                different
                                fusion strategies and setups are investigated. It outperforms recent range view based
                                lidar
                                approaches as well as all so far proposed fusion strategies and architectures.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </section>