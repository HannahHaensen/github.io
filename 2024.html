    <section>
        <br>
        <div class="container">

            <hr>
            <h4 id="2024">2024</h4>
            <hr>

            <br>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="360"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/asdf.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">ASDF: Assembly State Detection Utilizing Late Fusion by Integrating 6D Pose Estimation</span>
                    </a>
                    <br>
                    H. Schieber<span
                        style="font-weight: lighter">, S. Li, N. Correl, P. Beckerle, J. Kreimeier, and D. Roth</span>
                    <br>
                    <em></em>IEEE ISMAR 2024, Conference Paper
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseASDF" role="button"
                           aria-expanded="false" aria-controls="collapseASDF">
                            Abstract
                        </a>
                        <a class="btn btn-dark"
                           href="https://arxiv.org/pdf/2403.16400.pdf">Arxiv</a>
                    </p>
                    <div class="collapse" id="collapseASDF">
                        <div class="card card-body">
                            <p>
                               In medical and industrial domains, providing guidance for assembly processes is critical
                                to ensure efficiency and safety. Errors in assembly can lead to significant consequences
                                such as extended surgery times, and prolonged manufacturing or maintenance times in industry.
                                Assembly scenarios can benefit from in-situ AR visualization to provide guidance,
                                reduce assembly times and minimize errors. To enable in-situ visualization 6D pose estimation can be leveraged.
                                Existing 6D pose estimation techniques primarily focus on individual objects and static captures.
                                However, assembly scenarios have various dynamics including occlusion during assembly and
                                dynamics in the assembly objects appearance. Existing work, combining object detection/
                                6D pose estimation and assembly state detection focuses either on pure deep learning-based approaches,
                                or limit the assembly state detection to building blocks. To address the challenges of 6D pose estimation
                                in combination with assembly state detection, our approach ASDF builds upon the strengths of
                                YOLOv8, a real-time capable object detection framework. We extend this framework, refine the
                                object pose and fuse pose knowledge with network-detected pose information. Utilizing our late fusion in our Pose2State module results in refined 6D pose estimation and assembly state detection. By combining both pose and state information, our Pose2State module predicts the final assembly state with precision. Our evaluation on our \ac{asdf} dataset shows that our Pose2State module leads to an improved assembly state detection and that the improvement of the assembly state further leads to a more robust 6D pose estimation. Moreover, on the GBOT dataset, we outperform the pure deep learning-based network, and even outperform the hybrid and pure tracking-based approaches.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="360"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/teaser_demo.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">Visual Guidance for Assembly Processes</span>
                    </a> 
                    <br>
                    <span
                        style="font-weight: lighter"> J. Kreimeier,</span> H. Schieber, <span
                        style="font-weight: lighter"> S. Li, A. Martin-Gomez, and D. Roth</span>
                    <br>
                    <em></em>IEEE ISMAR 2024, Demo Paper
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseDemo" role="button"
                           aria-expanded="false" aria-controls="collapseDemo">
                            Abstract
                        </a>
                    </p>
                    <div class="collapse" id="collapseDemo">
                        <div class="card card-body">
                            <p>
                              Augmented reality (AR) can improve users' efficiency in various tasks.
                                AR allows to guide a user with superimposed information, for example,
                                during an assembly process. While paper-based assembly instructions are
                                cumbersome and time-consuming, deep-learning driven AR-based instructions can dynamically
                                adapt to the assembly scene and augmented 3D information over the physical objects (in-situ).
                                We present KARVIMIO, an AR assembly guidance application for instruments based on 3D printed parts
                                as reproducible testbed. Our approach utilizes purely synthetic training data for pose estimation
                                to allow an easy generalization of the approach to new assembly groups and other areas of use.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/h6d.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">HouseCat6D--A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</span>
                    </a>
                    <br>
                    <span style="font-weight: lighter">H. Jung*, G. Zhai*, S.-C. Wu*, P. Ruhkamp*, </span>H. Schieber*<span
                        style="font-weight: lighter">, P. Wang, G. Rizzoli, H. Zhao, S. D. Meier, D. Roth,
                    N. Navab and B. Busam,  *denotes equal contribution</span>
                    <br>
                    <em></em>CVPR, Highlight, 2024
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseHousecat6d" role="button"
                           aria-expanded="false" aria-controls="collapseHousecat6d">
                            Abstract
                        </a>
                        <a class="btn btn-dark"
                           href="https://arxiv.org/pdf/2212.10428.pdf">Arxiv</a>
                        <a class="btn btn-dark"
                           href="https://sites.google.com/view/housecat6d">Website</a>
                        <a class="btn btn-dark"
                           href="https://sites.google.com/view/housecat6d/dataset">Dataset</a>
                        <a class="btn btn-dark"
                           href="https://github.com/Junggy/HouseCat6D">Toolbox</a>
                    </p>
                    <div class="collapse" id="collapseHousecat6d">
                        <div class="card card-body">
                            <p>
                                Estimating 6D object poses is a major challenge in 3D computer vision. Building on
                                successful instance-level approaches, research is shifting towards category-level pose
                                estimation for practical applications. Current category-level datasets, however, fall
                                short in annotation quality and pose variety. Addressing this, we introduce HouseCat6D,
                                a new category-level 6D pose dataset. It features 1) multi-modality with Polarimetric
                                RGB and Depth (RGBD+P), 2) encompasses 194 diverse objects across 10 household
                                categories, including two photometrically challenging ones, and 3) provides high-quality
                                pose annotations with an error range of only 1.35 mm to 1.74 mm. The dataset also
                                includes 4) 41 large-scale scenes with comprehensive viewpoint and occlusion coverage,
                                5) a checkerboard-free environment, and 6) dense 6D parallel-jaw robotic grasp
                                annotations. Additionally, we present benchmark results for leading category-level pose
                                estimation networks.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="assets/img/teaser_proposal.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">
                        GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance
                    </span>

                    <br>
                    <span style="font-weight: lighter"> S. Li,</span> <span>H. Schieber</span> <span
                        style="font-weight: lighter">, B. Egger, J. Kreimeier and D. Roth</span>
                    <br>
                    <em></em>IEEE VR 2024, Conference Paper
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseGBOT" role="button"
                           aria-expanded="false" aria-controls="collapseGBOT">
                            Abstract
                        </a>
                        <a class="btn btn-dark"
                           href="https://arxiv.org/abs/2402.07677">Arxiv</a>
                        <a class="btn btn-dark"
                           href="https://github.com/roth-hex-lab/gbot">GitHub</a>

                    </p>
                    <div class="collapse" id="collapseGBOT">
                        <div class="card card-body">
                            <p>
                                Guidance for assemblable parts is a promising field for augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real time. Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts. In this regard, occlusions by the user's hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking. To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses. The tracking through various assembly states is achieved by our novel multi-state assembly graph. We update the multi-state assembly graph by utilizing the relative poses of the individual assembly parts. Linking the individual objects in this graph enables more robust object tracking during the assembly process. For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a benchmark for future work. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance. Dataset and code will be made publically available.
                            </p>
                        </div>
                    </div>

                </div>
            </div>
            <br>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="assets/img/handreha2.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">
                        Towards Continuous Patient Care with Remote Guided VR-Therapy (Poster)
                    </span>

                    <br>
                    <span style="font-weight: lighter"> J. Kreimeier, </span> <span>H. Schieber</span> <span
                        style="font-weight: lighter">, N. Lewis, M. Smietana, J. Reithmeier, V. Cnejevici, P. Prasad, A. Eid, M. Maier, D. Roth</span>
                    <br>
                    <em></em>IEEE VR 2024, Poster
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseVRReha" role="button"
                           aria-expanded="false" aria-controls="collapseVRReha">
                            Abstract
                        </a>
                    </p>
                    <div class="collapse" id="collapseVRReha">
                        <div class="card card-body">
                            <p>

                            </p>
                        </div>
                    </div>

                </div>
            </div>
            <br>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://hannahhaensen.github.io/mr-sense/images/system.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">MR-Sense: A Mixed Reality Environment Search Assistant for Blind and Visually Impaired People</span>
                    </a>
                    <br>
                    <span>H. Schieber</span>, <span style="font-weight: lighter">C. Kleinbeck, L. Theelke, M. Kraft, J. Kreimeier and D. Roth</span>
                    <br>
                    <em>IEEE AIxVR, </em>2024
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseMRSense" role="button"
                           aria-expanded="false" aria-controls="collapseMRSense">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/document/10445567">IEEE</a>
                        <a class="btn btn-dark" href="https://hannahhaensen.github.io/mr-sense/">GitHub/Website</a>
                    </p>
                    <div class="collapse" id="collapseMRSense">
                        <div class="card card-body">
                            <p>
                                Search tasks can be challenging for blind or visually impaired people. To determine an
                                object's
                                location and to navigate there, they often rely on the limited sensory capabilities of a
                                white
                                cane,
                                search haptically, or ask for help. We introduce MR-Sense, a mixed reality assistant to
                                support
                                search and navigation tasks. The system is designed in a participatory fashion and
                                utilizes
                                sensory
                                data of a standalone mixed reality head-mounted display to perform deep learning-driven
                                object
                                recognition and environment mapping. The user is supported in object search tasks via
                                spatially
                                mapped audio and vibrotactile feedback. We conducted a preliminary user study including
                                ten
                                blind or
                                visually impaired participants and a final user evaluation with thirteen blind or
                                visually
                                impaired
                                participants. The final study reveals that MR-Sense alone cannot replace the cane but
                                provides a
                                valuable addition in terms of usability and task load. We further propose a standardized
                                evaluation
                                setup for replicable studies and highlight relevant potentials and challenges fostering
                                future
                                work
                                towards employing technology in accessibility.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/timeline.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">Indoor Synthetic Data Generation: A Systematic Review</span>
                    </a>
                    <br>
                    <span>H. Schieber</span>, <span style="font-weight: lighter">K. C. Demir, C. Kleinbeck, S. H. Yang and D. Roth</span>
                    <br>
                    <em>Computer Vision and Image Understanding (CVIU), </em>2024
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn  btn-dark" data-bs-toggle="collapse" href="#collapseSynRev" role="button"
                           aria-expanded="false" aria-controls="collapseSynRev">
                            Abstract
                        </a>
                        <a class="btn btn-dark"
                           href="https://www.sciencedirect.com/science/article/pii/S1077314223002874">Elsevier
                            CVIU</a>
                        <a class="btn  btn-dark"
                           href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4528507">SSRN Preprint</a>
                        <a class="btn  btn-dark"
                           href="https://hannahhaensen.github.io/Synthetic-Data-A-Systematic-Review/">Website</a>
                    </p>
                    <div class="collapse" id="collapseSynRev">
                        <div class="card card-body">
                            <p>
                                Deep learning-based object recognition, 6D pose estimation, and semantic scene
                                understanding
                                requires a large amount of training data to achieve generalization. Time-consuming
                                annotation
                                processes, privacy, and security aspects lead to a scarcity of real-world datasets. To
                                overcome
                                this lack of data, synthetic data generation has been proposed, including multiple
                                facets in the
                                area of domain randomization to extend the data distribution. The objective of this
                                review is to
                                identify methods applied for synthetic data generation aiming to improve 6D pose
                                estimation,
                                object recognition, and semantic scene understanding in indoor scenarios. We further
                                review
                                methods used to extend the data distribution and discuss best practices to bridge the
                                gap
                                between synthetic and real-world data.
                            </p>
                        </div>
                    </div>
                </div>
            </div>

        </div>
    </section>