<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hannah Schieber</title>

    <meta name="author" content="Hannah Schieber">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <meta content="Hannah Schieber, PhD Student" property="og:title">
    <meta content="Hannah Schieber, PhD Student" property="description">
    <meta content="Hannah Schieber, personal webpage" property="description">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">

</head>

<body>
<div class="container">
    <section>
        <br>
        <br>
        <br>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <p class="name" style="text-align: center;">
                        Hannah Schieber, M. Sc., PhD Student @ FAU
                    </p>
                    <br>
                    <br>
                    <p>
                        I am a <b>PhD student</b> at the <b>
                        Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</b>. I am currently researching the topic of neural 3D content creation and guidance in 3D using extend reality.
                        <br>
                        Moreover, I am a <b>research assistant</b> at <b>Technical University of Munich (TUM), Germany</b>, assisting as tutor
                        at several courses like:
                        <a href="https://www.cs.cit.tum.de/camp/news/article/workshop-in-advanced-topics-in-3d-computer-vision-july-2023/">Advanced
                            Topics in 3D Computer Vision</a>
                        or <a
                            href="https://www.cs.cit.tum.de/camp/teaching/seminars/modern-computer-vision-methods-ws-2023-24/">Modern
                        Computer Vision Methods</a>
                        <br>
                        I am currently a visiting PhD student at the <b>University of Otago, New Zealand</b>. There I am researching the creation of 3D scenes in the Visual Computing Group under the supervision of Prof. Stefanie Zollmann.</p>
                    <p>
                        Prior that, I received my <b>bachelor’s degree</b> from <b>HS Aalen</b>, and my <b>master’s degree</b> from <b>TH
                        Ingolstadt</b>, both in computer science. During my studies, I gained experience in the industry
                        at <b>MAPAL Dr. Kress KG, c-Com GmbH, Carl Zeiss AG and AUDI AG</b>.
                    </p>
                    <p>
                        I am interested in computer vision, extended reality and in general in many things in life.
                    Besides being passionate about my PhD studies I like to cycling and bouldering.
                    </p>
                    <p>Interested in a research coperation? Feel free to contact me any time via e-mail:</p>
                    <p style="text-align:center">
                        <b>Contact:</b><br>
                        hannah dot schieber @ tum dot de &nbsp;/&nbsp;
                        <a href="https://scholar.google.com/citations?user=1tKoj0EAAAAJ&hl=de">Google Scholar</a>
                        &nbsp;/&nbsp;
                        <a href="https://twitter.com/hannah_haensen">Twitter</a> &nbsp;/&nbsp;
                        <a href="https://github.com/HannahHaensen/">Github</a>
                    </p>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12">
                    <a href="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/hannah-rgb-240x320.jpg">
                        <img style="width:100%;max-width:100%; border-radius: 50%;" alt="profile photo"
                             src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/hannah-rgb-240x320.jpg"
                             class="hoverZoomLink"></a>
                </div>

            </div>
        </div>
    </section>

    <section>
        <div class="container">

            <div class="row">
                <div class="col-12">
                    <h2>News</h2>
                    <ul>
                        <li>[03/2024] One paper accepted @ CVPR 2024</li>
                        <li>[02/2024] Research Visit at the University of Otago</li>
                        <li>[01/2024] One <a href="https://hannahhaensen.github.io/mr-sense/">paper</a> accepted at IEEE AIxVR 2024, presented in January</li>
                        <li>[12/2023] One <a href="https://www.sciencedirect.com/science/article/pii/S1077314223002874">paper</a> accepted at CVIU</li>
                    </ul>

                </div>
            </div>
            <div class="row">
                <div class="col-12">
                    <h2>Research</h2>
                    <b>Projects: </b>
                    Below you can find published research projects and at the bottom of the page some preprints.

                    <div class="row">
                        <div class="col-12">
                            <b>Services: </b>
                        </div>
                        <div class="col-lg-6 col-md-6 col-sm-12">
                            <i>Conferences</i>
                            <br>
                            &nbsp;&nbsp;I was reviewer at IEEE VR, IEEE ISMAR and the WICV'23 workshop at ICCV.
                            <br>
                        </div>
                        <div class="col-lg-6 col-md-6 col-sm-12">
                            <i>Journals</i>
                            <br>
                            &nbsp;&nbsp;For journals I was a reviewer at Springer IJCARS.
                        </div>
                    </div>
                    <div class="row">

                    </div>
                    <br>

                </div>
            </div>
        </div>
    </section>
    <section>
        <div class="container">
            <h4>2024</h4>
            <hr>

            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://hannahhaensen.github.io/mr-sense/images/system.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">MR-Sense: A Mixed Reality Environment Search Assistant for Blind and Visually Impaired People</span>
                    </a>
                    <br>
                    <span>H. Schieber</span>, <span style="font-weight: lighter">C. Kleinbeck, L. Theelke, M. Kraft, J. Kreimeier and D. Roth</span>
                    <br>
                    <em>IEEE AIxVR, </em>2024
                    <br>
                    <a href="https://hannahhaensen.github.io/mr-sense/">[GitHub/Website]</a>
                    <p>
                        Search tasks can be challenging for blind or visually impaired people. To determine an object's
                        location and to navigate there, they often rely on the limited sensory capabilities of a white
                        cane,
                        search haptically, or ask for help. We introduce MR-Sense, a mixed reality assistant to support
                        search and navigation tasks. The system is designed in a participatory fashion and utilizes
                        sensory
                        data of a standalone mixed reality head-mounted display to perform deep learning-driven object
                        recognition and environment mapping. The user is supported in object search tasks via spatially
                        mapped audio and vibrotactile feedback. We conducted a preliminary user study including ten
                        blind or
                        visually impaired participants and a final user evaluation with thirteen blind or visually
                        impaired
                        participants. The final study reveals that MR-Sense alone cannot replace the cane but provides a
                        valuable addition in terms of usability and task load. We further propose a standardized
                        evaluation
                        setup for replicable studies and highlight relevant potentials and challenges fostering future
                        work
                        towards employing technology in accessibility.
                    </p>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/timeline.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">Indoor Synthetic Data Generation: A Systematic Review</span>
                    </a>
                    <br>
                    <span>H. Schieber</span>, <span style="font-weight: lighter">K. C. Demir, C. Kleinbeck, S. H. Yang and D. Roth</span>
                    <br>
                    <em>Computer Vision and Image Understanding (CVIU), </em>2024
                    <br>
                    <a
                            href="https://www.sciencedirect.com/science/article/pii/S1077314223002874">[Elsevier
                        CVIU]</a>
                    <a
                            href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4528507">[SSRN Preprint]</a>
                    <a href="https://hannahhaensen.github.io/Synthetic-Data-A-Systematic-Review/">[Website]</a>
                    <p>
                        Deep learning-based object recognition, 6D pose estimation, and semantic scene understanding
                        requires a large amount of training data to achieve generalization. Time-consuming annotation
                        processes, privacy, and security aspects lead to a scarcity of real-world datasets. To overcome
                        this lack of data, synthetic data generation has been proposed, including multiple facets in the
                        area of domain randomization to extend the data distribution. The objective of this review is to
                        identify methods applied for synthetic data generation aiming to improve 6D pose estimation,
                        object recognition, and semantic scene understanding in indoor scenarios. We further review
                        methods used to extend the data distribution and discuss best practices to bridge the gap
                        between synthetic and real-world data.
                    </p>
                </div>
            </div>

        </div>
    </section>
    <!-- 2023 -->
    <section>

        <div class="container">
            <h4>2023</h4>
            <hr>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/F8pKEfDWsAA8TmL.jpg">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">A Modular Approach for 3D Reconstruction with Point Cloud Overlay (Poster)</span>

                    <br>
                    <span>H. Schieber</span>., <span style="font-weight: lighter">F. Schmid, M. UI-Hassan, S. Zollmann and D. Roth</span>
                    <br>
                    <em>Poster Session at ISMAR</em>, 2023
                    <br>
                    <a href="https://ieeexplore.ieee.org/document/10322162">[IEEE]</a>
                    <p>
                        We present a modular approach allowing the flexible exchange of the individual part, i.e.
                        the camera or SLAM algorithm. This work presents results from a pilot study involving five
                        participants to gain an impression of what kind of visualization type would be preferred and
                        whether the point cloud overlay would assist the user in recognizing changes in the
                        surroundings. The point cloud overlay enabled the participants to perceive more changes. The
                        pilot study revealed that 60% of the participants showed a preference for the point cloud
                        overlay over the pure mesh representation.
                    </p>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20145018.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <span class="papertitle">Injured Avatars: The Impact of Embodied Anatomies and Virtual Injuries on Well-being and Performance</span>

                    <br>
                    <span style="font-weight: lighter">C. Kleinbeck, </span>H. Schieber<span style="font-weight: lighter">, J. Kreimeier, A. Martin-Gomez, M. Unberath and D. Roth
                </span><br>
                    <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2023
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10269734">[IEEE TVCG]</a>
                    <p></p>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20144745.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <span class="papertitle">Deep Learning in Surgical Workflow Analysis: A Review of Phase and Step Recognition</span>

                    <br>
                    <span style="font-weight: lighter">Demir, K. C.,</span>  Schieber, H., <span style="font-weight: lighter">Weise, T. Roth, D., May, M., Maier, A., & Yang, S. H.
                </span><br>
                    <em>IEEE Journal of Biomedical and Health Informatics</em>, 2023
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10238472">[IEEE]</a>
                    <p></p>
                </div>
            </div>
        </div>
    </section>
    <!-- 2022 -->
    <section>
        <div class="container">
            <h4>2022</h4>
            <hr>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="50%"
                         src="https://user-images.githubusercontent.com/22636930/250826756-42a2da46-a9e9-41e6-83e8-c01f7bca38ba.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">ARTFM: Augmented Reality Visualization of Tool Functionality Manuals in Operating Rooms. (Poster)</span>
                    </a>
                    <br>
                    <span style="font-weight: lighter">Kleinbeck, C.,</span> <span>Schieber, H.</span><span style="font-weight: lighter">, Andress, S., Krautz, C., & Roth, D.
                </span><br>
                    <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                        (pp. 736-737). IEEE.</em>
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9757491">[IEEE]</a>
                    <p>
                        Error-free surgical procedures are crucial for a patient's health. However, with the
                        increasing complexity and variety of surgical instruments, it is difficult for clinical
                        staff to acquire detailed assembly and usage knowledge leading to errors in process and
                        preparation steps. Yet, the gold standard in retrieving necessary information when problems
                        occur is to get the paperbased manual. Reading through the necessary instructions is
                        time-consuming and decreases care quality. We propose ARTFM, a process integrated manual,
                        highlighting the correct parts needed, their location, and step-by-step instructions to
                        combine the instrument using an augmented reality head-mounted display.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="50%"
                         src="https://user-images.githubusercontent.com/22636930/250825228-2340df9b-920a-4378-8fa1-956ba771fd79.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">A Mixed Reality Guidance System for Blind and Visually Impaired People (Poster)</span>

                    <br>
                    <span>Schieber, H.</span><span style="font-weight: lighter">, Kleinbeck, C., Pradel, C., Theelke, L., & Roth, D.</span>
                    <br>
                    <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                        (pp. 726-727). IEEE.</em>, 2022
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9757681">[IEEE]</a>
                    <p></p>
                    <p>
                        Persons affected by blindness or visual impairments are challenged by spatially
                        understanding unfamiliar environments. To obtain such understanding, they have to sense
                        their environment closely and carefully. Especially objects outside the sensing area of
                        analog assistive devices, such as a white cane, are simply not perceived and can be the
                        cause of collisions. This project proposes a mixed reality guidance system that aims at
                        preventing such problems. We use object detection and the 3D sensing capabilities of a mixed
                        reality head mounted device to inform users about their spatial surroundings.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://user-images.githubusercontent.com/22636930/170203786-c1c6de02-5314-4275-bd36-ae655670f4b5.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation</span>

                    <br>
                    <span>H. Schieber*</span>, <span style="font-weight: lighter">F. Duerr*, T. Schoen and J. Beyerer, *denotes equal contribution</span>
                    <br>
                    <em>Intelligent Vehicles Symposium (IV), Aachen, Germany</em>, 2022
                    <br>
                    <a href="https://ieeexplore.ieee.org/document/9827113">[IEEE]</a>

                    <a
                            href="https://arxiv.org/pdf/2205.13629.pdf">[Arxiv]</a>
                    <a href="https://hannahhaensen.github.io/pyfu/">[Website]</a>
                    <p>
                        Robust environment perception for autonomous vehicles is a tremendous challenge, which makes a
                        diverse sensor set with e.g. camera, lidar and radar crucial. In the process of understanding
                        the recorded sensor data, 3D semantic segmentation plays an important role. Therefore, this work
                        presents a pyramid-based deep fusion architecture for lidar and camera to improve 3D semantic
                        segmentation of traffic scenes. Individual sensor backbones extract feature maps of camera
                        images and lidar point clouds. A novel Pyramid Fusion Backbone fuses these feature maps at
                        different scales and combines the multimodal features in a feature pyramid to compute valuable
                        multimodal, multi-scale features. The Pyramid Fusion Head aggregates these pyramid features and
                        further refines them in a late fusion step, incorporating the final features of the sensor
                        backbones. The approach is evaluated on two challenging outdoor datasets and different fusion
                        strategies and setups are investigated. It outperforms recent range view based lidar approaches
                        as well as all so far proposed fusion strategies and architectures.
                    </p>
                </div>
            </div>
        </div>

    </section>
    <!-- Preprints -->
    <section>
        <div class="container">
            <h4>Preprints</h4>
            <hr>
             <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="assets/img/Screenshot 2024-02-26 010901.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">
                        GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance
                    </span>

                    <br>
                    <span style="font-weight: lighter"> S. Li,</span> <span>H. Schieber</span> <span style="font-weight: lighter">, B. Egger, J. Kreimeier and D. Roth</span>
                    <br>
                    <em></em>2023
                    <br>
                    <a
                            href="https://arxiv.org/abs/2402.07677">[Arxiv]</a>
                    <p>
                        Guidance for assemblable parts is a promising field for augmented reality. Augmented reality assembly guidance requires 6D object poses of target objects in real time. Especially in time-critical medical or industrial settings, continuous and markerless tracking of individual parts is essential to visualize instructions superimposed on or next to the target object parts. In this regard, occlusions by the user's hand or other objects and the complexity of different assembly states complicate robust and real-time markerless multi-object tracking. To address this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The real-time markerless multi-object tracking is initialized via 6D pose estimation and updates the graph-based assembly poses. The tracking through various assembly states is achieved by our novel multi-state assembly graph. We update the multi-state assembly graph by utilizing the relative poses of the individual assembly parts. Linking the individual objects in this graph enables more robust object tracking during the assembly process. For evaluation, we introduce a synthetic dataset of publicly available and 3D printable assembly assets as a benchmark for future work. Quantitative experiments in synthetic data and further qualitative study in real test data show that GBOT can outperform existing work towards enabling context-aware augmented reality assembly guidance. Dataset and code will be made publically available.

                    </p>
                </div>
            </div>

            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://user-images.githubusercontent.com/22636930/231704527-8c070d6b-0ac8-4432-9bd2-17725a04d191.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing Diverse Intrinsic and Extrinsic Camera Parameters</span>

                    <br>
                    <span>H. Schieber</span>, <span style="font-weight: lighter">F. Deuser, B. Egger, N. Oswald and D. Roth</span>
                    <br>
                    <em></em>2023
                    <br>
                    <a
                            href="https://arxiv.org/pdf/2303.09412.pdf">[Arxiv]</a>
                    <a
                            href="https://hannahhaensen.github.io/nerftrinsic_four/">[GitHub]</a>
                    <p>
                        We utilize Gaussian Fourier features to estimate extrinsic camera parameters and dynamically
                        predict varying intrinsic camera parameters through the supervision of the projection error.
                        Our approach outperforms existing joint optimization methods on LLFF and BLEFF. In addition
                        to these existing datasets, we introduce a new dataset called iFF with varying intrinsic
                        camera parameters. NeRFtrinsic Four is a step forward in joint optimization NeRF-based view
                        synthesis and enables more realistic and flexible rendering in real-world scenarios with
                        varying camera parameters.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="80%"
                         src="https://github-production-user-asset-6210df.s3.amazonaws.com/22636930/272610460-e1e407ef-4249-4e5e-bc38-486e08204548.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</span>
                    </a>
                    <br>
                    <span style="font-weight: lighter">N. Schischka*</span>, <span>H. Schieber*</span>, <span style="font-weight: lighter">M. A. Karaoglu*, M. Görgülü*, F. Grötzner, A. Ladikos, D. Roth, N.
                    Navab and B. Busam,  *denotes equal contribution</span>
                    <br>
                    <em></em>2023
                    <br>
                    <a
                            href="https://arxiv.org/pdf/2309.08927.pdf">[Arxiv]</a>
                    <a
                            href="https://hannahhaensen.github.io/DynaMoN/">[GitHub/Website]</a>
                    <p>
                        Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera poses.
                        These are often hard to retrieve with existing structure-from-motion (SfM) pipelines as both
                        camera and scene content can change. We propose DynaMoN that leverages simultaneous
                        localization and mapping (SLAM) jointly with motion masking to handle dynamic scene content.
                        Our robust SLAM-based tracking module significantly accelerates the training process of the
                        dynamic NeRF while improving the quality of synthesized views at the same time. Extensive
                        experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's iPhone dataset,
                        three real-world datasets, shows the advantages of DynaMoN both for camera pose estimation
                        and novel view synthesis.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="260"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/h6d.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">HouseCat6D--A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</span>
                    </a>
                    <br>
                    <span style="font-weight: lighter">H. Jung*, S.-C. Wu*, P. Ruhkamp*, </span>H. Schieber*<span style="font-weight: lighter">, P. Wang, G. Rizzoli, H. Zhao, S. D. Meier, D. Roth,
                    N. Navab and B. Busam,  *denotes equal contribution</span>
                    <br>
                    <em></em>2023
                    <br>
                    <a
                            href="https://arxiv.org/pdf/2212.10428.pdf">[Arxiv]</a>
                    <a
                            href="https://sites.google.com/view/housecat6d">[Website]</a>
                    <a
                            href="https://sites.google.com/view/housecat6d/dataset">[Dataset]</a>
                    <a
                            href="https://github.com/Junggy/HouseCat6D">[Toolbox]</a>
                    <p></p>
                </div>
            </div>
        </div>
    </section>
    <section>
        <br>
        Copyright @ Hannah Schieber | This is my personal webpage.
        <br>
        <br>
        <br>
    </section>
</div>


</body>
</html>
