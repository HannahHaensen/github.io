<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hannah Schieber</title>

    <meta name="author" content="Hannah Schieber">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <meta content="Hannah Schieber, PhD Student" property="og:title">
    <meta content="Hannah Schieber, PhD Student" property="description">
    <meta content="Hannah Schieber, personal webpage" property="description">

</head>

<body>
<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr style="padding:0px">
        <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr style="padding:0px">
                    <td style="padding:2.5%;width:63%;vertical-align:middle">
                        <p class="name" style="text-align: center;">
                            Hannah Schieber, M. Sc.
                        </p>
                        <p>
                            I am a PhD student at the Department of Artificial Intelligence in Biomedical Engineering,
                            Friedrich-Alexander University Erlangen-Nürnberg (FAU). I am currently researching the topic
                            of smart remote extended reality teleconsultation and product guidance.
                        </p>
                        <p>
                            Moreover, I am an research assistant at Technical University of Munich, assisting as tutor
                            at several courses like:
                            <a href="https://www.cs.cit.tum.de/camp/news/article/workshop-in-advanced-topics-in-3d-computer-vision-july-2023/">Advanced
                                Topics in 3D Computer Vision</a>
                            or <a
                                href="https://www.cs.cit.tum.de/camp/teaching/seminars/modern-computer-vision-methods-ws-2023-24/">Modern
                            Computer Vision Methods</a>
                        </p>
                        <p>
                            Prior that, I received my bachelor’s degree from HS Aalen, and my master’s degree from TH
                            Ingolstadt, both in computer science. During my studies, I gained experience in the industry
                            at MAPAL Dr. Kress KG, c-Com GmbH, Carl Zeiss AG and AUDI AG.
                        </p>
                        <p>Interested in a research coperation? Feel free to contact me any time via e-mail:</p>
                        <p style="text-align:center">
                            hannah dot schieber @ tum dot de &nbsp;/&nbsp;
                            <a href="https://scholar.google.com/citations?user=1tKoj0EAAAAJ&hl=de">Google Scholar</a>
                            &nbsp;/&nbsp;
                            <a href="https://twitter.com/hannah_haensen">Twitter</a> &nbsp;/&nbsp;
                            <a href="https://github.com/HannahHaensen/">Github</a>
                        </p>
                    </td>
                    <td style="padding:2.5%;width:40%;max-width:40%">
                        <a href="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/hannah-rgb-240x320.jpg">
                            <img style="width:100%;max-width:100%" alt="profile photo"
                                 src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/hannah-rgb-240x320.jpg"
                                 class="hoverZoomLink"></a>
                    </td>
                </tr>
                </tbody>
            </table>
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:100%;vertical-align:middle">
                        <h2>Research</h2>
                        <p>
                            I am interested in computer vision, extended reality and in general in many things in life.
                            Besides being passionate about my PhD studies I like to go cycling or go bouldering.
                        </p>
                        <p>
                            <b>Projects: </b>
                            Below you can find published research projects and at the bottom of the page some preprints.
                        </p>
                        <p>
                            <b>Services: </b>

                            I was reviewer at IEEE VR, IEEE ISMAR and the WICV workshop at ICCV.
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>

            <h4>2023</h4>
            <hr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a>
                            <span class="papertitle">A Modular Approach for 3D Reconstruction with Point Cloud Overlay (Poster)</span>
                        </a>
                        <br>
                        H. Schieber, F. Schmid, M. UI-Hassan, S. Zollmann and D. Roth
                        <br>
                        <em>Poster Session at ISMAR</em>, 2023
                        <br>
                        <a href="https://ieeexplore.ieee.org/document/10322162">[IEEE]</a>
                        <p>
                            We present a modular approach allowing the flexible exchange of the individual part, i.e. the camera or SLAM algorithm. This work presents results from a pilot study involving five participants to gain an impression of what kind of visualization type would be preferred and whether the point cloud overlay would assist the user in recognizing changes in the surroundings. The point cloud overlay enabled the participants to perceive more changes. The pilot study revealed that 60% of the participants showed a preference for the point cloud overlay over the pure mesh representation.
                        </p>
                    </td>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="260"
                             src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/F8pKEfDWsAA8TmL.jpg">
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10269734">
                            <span class="papertitle">Injured Avatars: The Impact of Embodied Anatomies and Virtual Injuries on Well-being and Performance</span>
                        </a>
                        <br>
                        C. Kleinbeck, H. Schieber, J. Kreimeier, A. Martin-Gomez, M. Unberath and D. Roth
                        <br>
                        <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2023
                        <br>
                        <a href="https://ieeexplore.ieee.org/abstract/document/10269734">[IEEE TVCG]</a>
                        <p></p>
                    </td>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="260"
                             src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20145018.png">
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a href="https://ieeexplore.ieee.org/abstract/document/10238472">
                            <span class="papertitle">Deep Learning in Surgical Workflow Analysis: A Review of Phase and Step Recognition</span>
                        </a>
                        <br>
                        Demir, K. C., Schieber, H., WeiseRoth, T., May, M., Maier, A., & Yang, S. H.
                        <br>
                        <em>IEEE Journal of Biomedical and Health Informatics</em>, 2023
                        <br>
                        <a href="https://ieeexplore.ieee.org/abstract/document/10238472">[IEEE]</a>
                        <p></p>
                    </td>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="260"
                             src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20144745.png">
                    </td>
                </tr>
                </tbody>
            </table>

            <h4>2022</h4>
            <hr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="160"
                             src="https://user-images.githubusercontent.com/22636930/250826756-42a2da46-a9e9-41e6-83e8-c01f7bca38ba.png">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a>
                            <span class="papertitle">ARTFM: Augmented Reality Visualization of Tool Functionality Manuals in Operating Rooms. (Poster)</span>
                        </a>
                        <br>
                        Kleinbeck, C., Schieber, H., Andress, S., Krautz, C., & Roth, D.
                        <br>
                        <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                            (pp. 736-737). IEEE.</em>
                        <br>
                        <a href="https://ieeexplore.ieee.org/abstract/document/9757491">[IEEE]</a>
                        <p>
                            Error-free surgical procedures are crucial for a patient's health. However, with the increasing complexity and variety of surgical instruments, it is difficult for clinical staff to acquire detailed assembly and usage knowledge leading to errors in process and preparation steps. Yet, the gold standard in retrieving necessary information when problems occur is to get the paperbased manual. Reading through the necessary instructions is time-consuming and decreases care quality. We propose ARTFM, a process integrated manual, highlighting the correct parts needed, their location, and step-by-step instructions to combine the instrument using an augmented reality head-mounted display.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="160"
                             src="https://user-images.githubusercontent.com/22636930/250825228-2340df9b-920a-4378-8fa1-956ba771fd79.png">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a>
                            <span class="papertitle">A Mixed Reality Guidance System for Blind and Visually Impaired People (Poster)</span>
                        </a>
                        <br>
                        Schieber, H., Kleinbeck, C., Pradel, C., Theelke, L., & Roth, D.
                        <br>
                        <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                            (pp. 726-727). IEEE.</em>, 2022
                        <br>
                        <a href="https://ieeexplore.ieee.org/abstract/document/9757681">[IEEE]</a>
                        <p></p>
                        <p>
                            Persons affected by blindness or visual impairments are challenged by spatially
                            understanding unfamiliar environments. To obtain such understanding, they have to sense
                            their environment closely and carefully. Especially objects outside the sensing area of
                            analog assistive devices, such as a white cane, are simply not perceived and can be the
                            cause of collisions. This project proposes a mixed reality guidance system that aims at
                            preventing such problems. We use object detection and the 3D sensing capabilities of a mixed
                            reality head mounted device to inform users about their spatial surroundings.
                        </p>
                    </td>
                </tr>
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="160"
                             src="https://user-images.githubusercontent.com/22636930/250823795-80184232-0e85-4b26-967a-fa62b517adb3.png">
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a>
                            <span class="papertitle">Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation</span>
                        </a>
                        <br>
                        H. Schieber, F. Duerr, T. Schoen and J. Beyerer
                        <br>
                        <em>Intelligent Vehicles Symposium (IV), Aachen, Germany</em>, 2022
                        <br>
                        <a href="https://ieeexplore.ieee.org/document/9827113">[IEEE]</a>

                        <a
                            href="https://arxiv.org/pdf/2205.13629.pdf">[Arxiv]</a>
                        <p></p>
                    </td>
                </tr>
                </tbody>
            </table>

            <h4>Preprints</h4>
            <hr>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>

                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a>
                            <span class="papertitle">Indoor Synthetic Data Generation: A Systematic Review</span>
                        </a>
                        <br>
                       H Schieber, K. C. Demir, C. Kleinbeck, S. H. Yang and D. Roth
                        <br>
                        <em></em>2023
                        <br>
                        <a
                            href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4528507">[SSRN]</a>
                        <p>
                            Deep learning-based object recognition, 6D pose estimation, and semantic scene understanding requires a large amount of training data to achieve generalization. Time-consuming annotation processes, privacy, and security aspects lead to a scarcity of real-world datasets. To overcome this lack of data, synthetic data generation has been proposed, including multiple facets in the area of domain randomization to extend the data distribution. The objective of this review is to identify methods applied for synthetic data generation aiming to improve 6D pose estimation, object recognition, and semantic scene understanding in indoor scenarios. We further review methods used to extend the data distribution and discuss best practices to bridge the gap between synthetic and real-world data.
                        </p>
                    </td>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="260"
                             src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/timeline.png">
                    </td>
                </tr>
                <tr>

                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a>
                            <span class="papertitle">NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing Diverse Intrinsic and Extrinsic Camera Parameters</span>
                        </a>
                        <br>
                        H. Schieber, F. Deuser, B. Egger, N. Oswald and D. Roth
                        <br>
                        <em></em>2023
                        <br>
                        <a
                            href="https://arxiv.org/pdf/2303.09412.pdf">[Arxiv]</a>
                        <a
                            href="https://hannahhaensen.github.io/nerftrinsic_four/">[GitHub]</a>
                        <p>
                            We utilize Gaussian Fourier features to estimate extrinsic camera parameters and dynamically predict varying intrinsic camera parameters through the supervision of the projection error. Our approach outperforms existing joint optimization methods on LLFF and BLEFF. In addition to these existing datasets, we introduce a new dataset called iFF with varying intrinsic camera parameters. NeRFtrinsic Four is a step forward in joint optimization NeRF-based view synthesis and enables more realistic and flexible rendering in real-world scenarios with varying camera parameters.
                        </p>
                    </td>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="260"
                             src="https://user-images.githubusercontent.com/22636930/231704527-8c070d6b-0ac8-4432-9bd2-17725a04d191.png">
                    </td>
                </tr>
                <tr>

                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a>
                            <span class="papertitle">DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</span>
                        </a>
                        <br>
                        M. A. Karaoglu, H. Schieber, N. Schischka, M. Görgülü, F. Grötzner, A. Ladikos, D. Roth, N. Navab and B. Busam
                        <br>
                        <em></em>2023
                        <br>
                        <a
                            href="https://arxiv.org/pdf/2309.08927.pdf">[Arxiv]</a>
                        <a
                            href="https://hannahhaensen.github.io/DynaMoN/">[GitHub]</a>
                        <p>
                            Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera poses. These are often hard to retrieve with existing structure-from-motion (SfM) pipelines as both camera and scene content can change. We propose DynaMoN that leverages simultaneous localization and mapping (SLAM) jointly with motion masking to handle dynamic scene content. Our robust SLAM-based tracking module significantly accelerates the training process of the dynamic NeRF while improving the quality of synthesized views at the same time. Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's iPhone dataset, three real-world datasets, shows the advantages of DynaMoN both for camera pose estimation and novel view synthesis.
                        </p>
                    </td>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="260"
                             src="https://github-production-user-asset-6210df.s3.amazonaws.com/22636930/272610460-e1e407ef-4249-4e5e-bc38-486e08204548.png">
                    </td>
                </tr>
                <tr>

                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <a>
                            <span class="papertitle">HouseCat6D--A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</span>
                        </a>
                        <br>
                        H. Jung, S.-C. Wu, P. Ruhkamp, H. Schieber, P. Wang, G. Rizzoli, H. Zhao, S. D. Meier, D. Roth, N. Navab and B. Busam
                        <br>
                        <em></em>2023
                        <br>
                        <a
                            href="https://arxiv.org/pdf/2212.10428.pdf">[Arxiv]</a>
                        <p></p>
                    </td>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <img width="260"
                             src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/h6d.png">
                    </td>
                </tr>
                </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
                <tbody>
                <tr>
                    <td style="padding:0px">
                        <br>
                        <p style="text-align:right;font-size:small;">
                            This website was originally created by: <a
                                href="https://github.com/jonbarron/jonbarron_website">source code</a>
                        </p>
                    </td>
                </tr>
                </tbody>
            </table>
        </td>
    </tr>
</table>
</body>
</html>