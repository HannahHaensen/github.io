<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hannah Schieber</title>

    <meta name="author" content="Hannah Schieber">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <meta content="Hannah Schieber, PhD Student" property="og:title">
    <meta content="Hannah Schieber, PhD Student" property="description">
    <meta content="Hannah Schieber, personal webpage" property="description">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-QWTKZyjpPEjISv5WaRU9OFeRpok6YctnYmDr5pNlyT2bRjXh0JMhjY6hW+ALEwIH" crossorigin="anonymous">
    <style>
        body {
            font-size: medium;
        }

        p {
            font-size: medium;
        }

        b {
            font-size: medium;
        }

        a {
            font-size: large;
        }

    </style>
</head>

<body>
<div class="container">
    <section>
        <br>
        <br>
        <br>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <p class="name" style="text-align: center;">
                        Hannah Schieber, M. Sc., PhD Student @ FAU
                    </p>
                    <br>
                    <br>
                    <p>
                        I am a <b>PhD student</b> at the <b>
                        Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany</b>. I am currently researching
                        the topic of neural 3D content creation and guidance in 3D using extend reality.
                        <br>
                        Moreover, I am a <b>research assistant</b> at <b>Technical University of Munich (TUM),
                        Germany</b>, assisting as tutor
                        at several courses like:
                        <a href="https://www.cs.cit.tum.de/camp/news/article/workshop-in-advanced-topics-in-3d-computer-vision-july-2023/">Advanced
                            Topics in 3D Computer Vision</a>
                        or <a
                            href="https://www.cs.cit.tum.de/camp/teaching/seminars/modern-computer-vision-methods-ws-2023-24/">Modern
                        Computer Vision Methods</a>
                        <br>
                        I am currently a visiting PhD student at the <b>University of Otago, New Zealand</b>. There I am
                        researching the creation of 3D scenes in the Visual Computing Group under the supervision of
                        Prof. Stefanie Zollmann.</p>
                    <p>
                        Prior that, I received my <b>bachelor’s degree</b> from <b>HS Aalen</b>, and my <b>master’s
                        degree</b> from <b>TH
                        Ingolstadt</b>, both in computer science. During my studies, I gained experience in the industry
                        at <b>MAPAL Dr. Kress KG, c-Com GmbH, Carl Zeiss AG and AUDI AG</b>.
                    </p>
                    <p>
                        I am interested in computer vision, extended reality and in general in many things in life.
                        Besides being passionate about my PhD studies I like to cycling and bouldering.
                    </p>
                    <p>Interested in a research coperation? Feel free to contact me any time via e-mail:</p>
                    <p style="text-align:center">
                        <b>Contact:</b><br>
                        hannah dot schieber @ tum dot de &nbsp;/&nbsp;
                        <a href="https://scholar.google.com/citations?user=1tKoj0EAAAAJ&hl=de">Google Scholar</a>
                        &nbsp;/&nbsp;
                        <a href="https://twitter.com/hannah_haensen">Twitter</a> &nbsp;/&nbsp;
                        <a href="https://github.com/HannahHaensen/">Github</a>
                    </p>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12">
                    <a href="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/hannah-rgb-240x320.jpg">
                        <img style="width:100%;max-width:100%; border-radius: 50%;" alt="profile photo"
                             src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/hannah-rgb-240x320.jpg"
                             class="hoverZoomLink"></a>
                </div>

            </div>
        </div>
    </section>

    <section>
        <div class="container">

            <div class="row">
                <div class="col-12">
                    <h2>News</h2>
                    <ul>
                        <li>[03/2024] One paper and one poster accepted at IEEE VR 2024 (second author)</li>
                        <li>[03/2024] One paper accepted at CVPR 2024 (shared first author)</li>
                        <li>[02/2024] Research Visit at the University of Otago</li>
                        <li>[01/2024] One <a href="https://hannahhaensen.github.io/mr-sense/">paper</a> accepted at IEEE
                            AIxVR 2024, presented in January
                        </li>
                    </ul>

                </div>
            </div>
            <div class="row">
                <div class="col-12">
                    <h2>Information</h2>
                    <b>Projects: </b>
                    Below you can find published research projects and at the bottom of the page some preprints.

                    <div class="row">
                        <div class="col-12">
                            <b>Services: </b>
                        </div>
                        <div class="col-lg-6 col-md-6 col-sm-12">
                            <i>Conferences</i>
                            <br>
                            &nbsp;&nbsp;I was reviewer at IEEE VR, IEEE ISMAR and the WICV'23 workshop at ICCV.
                            <br>
                        </div>
                        <div class="col-lg-6 col-md-6 col-sm-12">
                            <i>Journals</i>
                            <br>
                            &nbsp;&nbsp;For journals I was a reviewer at Springer IJCARS.
                        </div>
                    </div>
                    <div class="row">

                    </div>
                    <br>

                </div>
            </div>
        </div>
    </section>
    <section>
        <br>
        <div class="container">

            <h2>Research</h2>
            <div class="row">
                <div class="col-12">
                    <a href="#Preprints"><b style="font-size: large">Preprints</b></a> | <a href="#2022"><b
                        style="font-size: large">2022</b></a> | <a href="#2023"><b style="font-size: large">2023</b></a>
                    | <a href="#2024"><b style="font-size: large">2024</b></a>
                    <br>
                    <br>
                </div>
            </div>
        </div>
    </section>
    <section>
        <br>
        <div class="container">
            <h4 id="2024">2024</h4>
            <hr>

            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://hannahhaensen.github.io/mr-sense/images/system.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">MR-Sense: A Mixed Reality Environment Search Assistant for Blind and Visually Impaired People</span>
                    </a>
                    <br>
                    <span>H. Schieber</span>, <span style="font-weight: lighter">C. Kleinbeck, L. Theelke, M. Kraft, J. Kreimeier and D. Roth</span>
                    <br>
                    <em>IEEE AIxVR, </em>2024
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseMRSense" role="button"
                           aria-expanded="false" aria-controls="collapseMRSense">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://hannahhaensen.github.io/mr-sense/">GitHub/Website</a>
                    </p>
                    <div class="collapse" id="collapseMRSense">
                        <div class="card card-body">
                            <p>
                                Search tasks can be challenging for blind or visually impaired people. To determine an
                                object's
                                location and to navigate there, they often rely on the limited sensory capabilities of a
                                white
                                cane,
                                search haptically, or ask for help. We introduce MR-Sense, a mixed reality assistant to
                                support
                                search and navigation tasks. The system is designed in a participatory fashion and
                                utilizes
                                sensory
                                data of a standalone mixed reality head-mounted display to perform deep learning-driven
                                object
                                recognition and environment mapping. The user is supported in object search tasks via
                                spatially
                                mapped audio and vibrotactile feedback. We conducted a preliminary user study including
                                ten
                                blind or
                                visually impaired participants and a final user evaluation with thirteen blind or
                                visually
                                impaired
                                participants. The final study reveals that MR-Sense alone cannot replace the cane but
                                provides a
                                valuable addition in terms of usability and task load. We further propose a standardized
                                evaluation
                                setup for replicable studies and highlight relevant potentials and challenges fostering
                                future
                                work
                                towards employing technology in accessibility.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/timeline.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">Indoor Synthetic Data Generation: A Systematic Review</span>
                    </a>
                    <br>
                    <span>H. Schieber</span>, <span style="font-weight: lighter">K. C. Demir, C. Kleinbeck, S. H. Yang and D. Roth</span>
                    <br>
                    <em>Computer Vision and Image Understanding (CVIU), </em>2024
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn  btn-dark" data-bs-toggle="collapse" href="#collapseSynRev" role="button"
                           aria-expanded="false" aria-controls="collapseSynRev">
                            Abstract
                        </a>
                        <a class="btn btn-dark"
                           href="https://www.sciencedirect.com/science/article/pii/S1077314223002874">Elsevier
                            CVIU</a>
                        <a class="btn  btn-dark"
                           href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4528507">SSRN Preprint</a>
                        <a class="btn  btn-dark"
                           href="https://hannahhaensen.github.io/Synthetic-Data-A-Systematic-Review/">Website</a>
                    </p>
                    <div class="collapse" id="collapseSynRev">
                        <div class="card card-body">
                            <p>
                                Deep learning-based object recognition, 6D pose estimation, and semantic scene
                                understanding
                                requires a large amount of training data to achieve generalization. Time-consuming
                                annotation
                                processes, privacy, and security aspects lead to a scarcity of real-world datasets. To
                                overcome
                                this lack of data, synthetic data generation has been proposed, including multiple
                                facets in the
                                area of domain randomization to extend the data distribution. The objective of this
                                review is to
                                identify methods applied for synthetic data generation aiming to improve 6D pose
                                estimation,
                                object recognition, and semantic scene understanding in indoor scenarios. We further
                                review
                                methods used to extend the data distribution and discuss best practices to bridge the
                                gap
                                between synthetic and real-world data.
                            </p>
                        </div>
                    </div>
                </div>
            </div>

        </div>
    </section>
    <!-- 2023 -->
    <section>
        <br>
        <div class="container">
            <h4 id="2023">2023</h4>
            <hr>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/F8pKEfDWsAA8TmL.jpg">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">A Modular Approach for 3D Reconstruction with Point Cloud Overlay (Poster)</span>

                    <br>
                    <span>H. Schieber</span>., <span style="font-weight: lighter">F. Schmid, M. UI-Hassan, S. Zollmann and D. Roth</span>
                    <br>
                    <em>Poster Session at ISMAR</em>, 2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn  btn-dark" data-bs-toggle="collapse" href="#collapseISMAR3D" role="button"
                           aria-expanded="false" aria-controls="collapseISMAR3D">
                            Abstract
                        </a>
                        <a class="btn  btn-dark" href="https://ieeexplore.ieee.org/document/10322162">IEEE</a>
                    </p>
                    <div class="collapse" id="collapseISMAR3D">
                        <div class="card card-body">
                            <p>
                                We present a modular approach allowing the flexible exchange of the individual part,
                                i.e.
                                the camera or SLAM algorithm. This work presents results from a pilot study involving
                                five
                                participants to gain an impression of what kind of visualization type would be preferred
                                and
                                whether the point cloud overlay would assist the user in recognizing changes in the
                                surroundings. The point cloud overlay enabled the participants to perceive more changes.
                                The
                                pilot study revealed that 60% of the participants showed a preference for the point
                                cloud
                                overlay over the pure mesh representation.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20145018.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <span class="papertitle">Injured Avatars: The Impact of Embodied Anatomies and Virtual Injuries on Well-being and Performance</span>

                    <br>
                    <span style="font-weight: lighter">C. Kleinbeck, </span>H. Schieber<span
                        style="font-weight: lighter">, J. Kreimeier, A. Martin-Gomez, M. Unberath and D. Roth
                </span><br>
                    <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseInjury" role="button"
                           aria-expanded="false" aria-controls="collapseInjury">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/abstract/document/10269734">IEEE
                            TVCG</a>
                    </p>
                    <div class="collapse" id="collapseInjury">
                        <div class="card card-body">
                            <p>
                                Human cognition relies on embodiment as a fundamental mechanism. Virtual avatars allow
                                users to experience the adaptation, control, and perceptual illusion of alternative
                                bodies. Although virtual bodies have medical applications in motor rehabilitation and
                                therapeutic interventions, their potential for learning anatomy and medical
                                communication remains underexplored. For learners and patients, anatomy, procedures, and
                                medical imaging can be abstract and difficult to grasp. Experiencing anatomies,
                                injuries, and treatments virtually through one's own body could be a valuable tool for
                                fostering understanding. This work investigates the impact of avatars displaying anatomy
                                and injuries suitable for such medical simulations. We ran a user study utilizing a
                                skeleton avatar and virtual injuries, comparing to a healthy human avatar as a baseline.
                                We evaluate the influence on embodiment, well-being, and presence with self-report
                                questionnaires, as well as motor performance via an arm movement task. Our results show
                                that while both anatomical representation and injuries increase feelings of eeriness,
                                there are no negative effects on embodiment, well-being, presence, or motor performance.
                                These findings suggest that virtual representations of anatomy and injuries are suitable
                                for medical visualizations targeting learning or communication without significantly
                                affecting users' mental state or physical control within the simulation.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20144745.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <span class="papertitle">Deep Learning in Surgical Workflow Analysis: A Review of Phase and Step Recognition</span>

                    <br>
                    <span style="font-weight: lighter">Demir, K. C.,</span> Schieber, H., <span
                        style="font-weight: lighter">Weise, T. Roth, D., May, M., Maier, A., & Yang, S. H.
                </span><br>
                    <em>IEEE Journal of Biomedical and Health Informatics</em>, 2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseRev" role="button"
                           aria-expanded="false" aria-controls="collapseRev">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/abstract/document/10238472">IEEE
                            JBHI</a>
                    </p>
                    <div class="collapse" id="collapseRev">
                        <div class="card card-body">
                            <p>
                                Objective: In the last two decades, there has been a growing interest in exploring
                                surgical procedures with statistical models to analyze operations at different semantic
                                levels. This information is necessary for developing context-aware intelligent systems,
                                which can assist the physicians during operations, evaluate procedures afterward or help
                                the management team to effectively utilize the operating room. The objective is to
                                extract reliable patterns from surgical data for the robust estimation of surgical
                                activities performed during operations. The purpose of this article is to review the
                                state-of-the-art deep learning methods that have been published after 2018 for analyzing
                                surgical workflows, with a focus on phase and step recognition. Methods: Three
                                databases, IEEE Xplore, Scopus, and PubMed were searched, and additional studies are
                                added through a manual search. After the database search, 343 studies were screened and
                                a total of 44 studies are selected for this review. Conclusion: The use of temporal
                                information is essential for identifying the next surgical action. Contemporary methods
                                used mainly RNNs, hierarchical CNNs, and Transformers to preserve long-distance temporal
                                relations. The lack of large publicly available datasets for various procedures is a
                                great challenge for the development of new and robust models. As supervised learning
                                strategies are used to show proof-of-concept, self-supervised, semi-supervised, or
                                active learning methods are used to mitigate dependency on annotated data. Significance:
                                The present study provides a comprehensive review of recent methods in surgical workflow
                                analysis, summarizes commonly used architectures, datasets, and discusses challenges.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- 2022 -->
    <section>
        <br>
        <div class="container">
            <h4 id="2022">2022</h4>
            <hr>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="50%"
                         src="https://user-images.githubusercontent.com/22636930/250826756-42a2da46-a9e9-41e6-83e8-c01f7bca38ba.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">ARTFM: Augmented Reality Visualization of Tool Functionality Manuals in Operating Rooms. (Poster)</span>
                    </a>
                    <br>
                    <span style="font-weight: lighter">Kleinbeck, C.,</span> <span>Schieber, H.</span><span
                        style="font-weight: lighter">, Andress, S., Krautz, C., & Roth, D.
                </span><br>
                    <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                        (pp. 736-737). IEEE.</em>
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseARTFM" role="button"
                           aria-expanded="false" aria-controls="collapseARTFM">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/abstract/document/9757491">IEEE</a>
                    </p>
                    <div class="collapse" id="collapseARTFM">
                        <div class="card card-body">
                            <p>
                                Error-free surgical procedures are crucial for a patient's health. However, with the
                                increasing complexity and variety of surgical instruments, it is difficult for clinical
                                staff to acquire detailed assembly and usage knowledge leading to errors in process and
                                preparation steps. Yet, the gold standard in retrieving necessary information when
                                problems
                                occur is to get the paperbased manual. Reading through the necessary instructions is
                                time-consuming and decreases care quality. We propose ARTFM, a process integrated
                                manual,
                                highlighting the correct parts needed, their location, and step-by-step instructions to
                                combine the instrument using an augmented reality head-mounted display.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="50%"
                         src="https://user-images.githubusercontent.com/22636930/250825228-2340df9b-920a-4378-8fa1-956ba771fd79.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">A Mixed Reality Guidance System for Blind and Visually Impaired People (Poster)</span>

                    <br>
                    <span>Schieber, H.</span><span style="font-weight: lighter">, Kleinbeck, C., Pradel, C., Theelke, L., & Roth, D.</span>
                    <br>
                    <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                        (pp. 726-727). IEEE.</em>, 2022
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseBlind" role="button"
                           aria-expanded="false" aria-controls="collapseBlind">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/abstract/document/9757681">[IEEE]</a>

                    </p>
                    <div class="collapse" id="collapseBlind">
                        <div class="card card-body">
                            <p>
                                Persons affected by blindness or visual impairments are challenged by spatially
                                understanding unfamiliar environments. To obtain such understanding, they have to sense
                                their environment closely and carefully. Especially objects outside the sensing area of
                                analog assistive devices, such as a white cane, are simply not perceived and can be the
                                cause of collisions. This project proposes a mixed reality guidance system that aims at
                                preventing such problems. We use object detection and the 3D sensing capabilities of a
                                mixed reality head mounted device to inform users about their spatial surroundings.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://user-images.githubusercontent.com/22636930/170203786-c1c6de02-5314-4275-bd36-ae655670f4b5.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation</span>

                    <br>
                    <span>H. Schieber*</span>, <span style="font-weight: lighter">F. Duerr*, T. Schoen and J. Beyerer, *denotes equal contribution</span>
                    <br>
                    <em>Intelligent Vehicles Symposium (IV), Aachen, Germany</em>, 2022
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseIV" role="button"
                           aria-expanded="false" aria-controls="collapseIV">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/document/9827113">IEEE</a>
                        <a class="btn btn-dark"
                           href="https://arxiv.org/pdf/2205.13629.pdf">Arxiv</a>
                        <a class="btn btn-dark" href="https://hannahhaensen.github.io/pyfu/">Website</a>
                    </p>
                    <div class="collapse" id="collapseIV">
                        <div class="card card-body">
                            <p>
                                Robust environment perception for autonomous vehicles is a tremendous challenge, which
                                makes a diverse sensor set with e.g. camera, lidar and radar crucial. In the process of
                                understanding the recorded sensor data, 3D semantic segmentation plays an important
                                role. Therefore,
                                this work presents a pyramid-based deep fusion architecture for lidar and camera to
                                improve 3D
                                semantic segmentation of traffic scenes. Individual sensor backbones extract feature
                                maps of
                                camera images and lidar point clouds. A novel Pyramid Fusion Backbone fuses these
                                feature maps
                                at different scales and combines the multimodal features in a feature pyramid to compute
                                valuable multimodal, multi-scale features. The Pyramid Fusion Head aggregates these
                                pyramid
                                features and further refines them in a late fusion step, incorporating the final
                                features of the
                                sensor backbones. The approach is evaluated on two challenging outdoor datasets and
                                different
                                fusion strategies and setups are investigated. It outperforms recent range view based
                                lidar
                                approaches as well as all so far proposed fusion strategies and architectures.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>

    </section>
    <!-- Preprints -->
    <section>
        <br>
        <div class="container">
            <h4 id="Preprints">Preprints</h4>
            <hr>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="assets/img/teaser_proposal.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">
                        GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted Assembly Guidance
                    </span>

                    <br>
                    <span style="font-weight: lighter"> S. Li,</span> <span>H. Schieber</span> <span
                        style="font-weight: lighter">, B. Egger, J. Kreimeier and D. Roth</span>
                    <br>
                    <em></em>2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseGBOT" role="button"
                           aria-expanded="false" aria-controls="collapseGBOT">
                            Abstract
                        </a>
                        <a class="btn btn-dark"
                           href="https://arxiv.org/abs/2402.07677">Arxiv</a>
                    </p>
                    <div class="collapse" id="collapseGBOT">
                        <div class="card card-body">
                            <p>
                                Guidance for assemblable parts is a promising field for augmented reality. Augmented
                                reality assembly guidance requires 6D object poses of target objects in real time.
                                Especially in
                                time-critical medical or industrial settings, continuous and markerless tracking of
                                individual parts is essential to visualize instructions superimposed on or next to the
                                target
                                object parts. In this regard, occlusions by the user's hand or other objects and the
                                complexity of
                                different assembly states complicate robust and real-time markerless multi-object
                                tracking. To address
                                this problem, we present Graph-based Object Tracking (GBOT), a novel graph-based
                                single-view RGB-D tracking approach. The real-time markerless multi-object tracking is
                                initialized
                                via 6D pose estimation and updates the graph-based assembly poses. The tracking through
                                various
                                assembly states is achieved by our novel multi-state assembly graph. We update the
                                multi-state assembly graph by utilizing the relative poses of the individual assembly
                                parts. Linking
                                the individual objects in this graph enables more robust object tracking during the
                                assembly
                                process. For evaluation, we introduce a synthetic dataset of publicly available and 3D
                                printable assembly assets as a benchmark for future work. Quantitative experiments in
                                synthetic
                                data and further qualitative study in real test data show that GBOT can outperform
                                existing work
                                towards enabling context-aware augmented reality assembly guidance. Dataset and code
                                will be
                                made publically available.
                            </p>
                        </div>
                    </div>

                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://user-images.githubusercontent.com/22636930/231704527-8c070d6b-0ac8-4432-9bd2-17725a04d191.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <span class="papertitle">NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing Diverse Intrinsic and Extrinsic Camera Parameters</span>
                    <br>
                    <span>H. Schieber</span>, <span style="font-weight: lighter">F. Deuser, B. Egger, N. Oswald and D. Roth</span>
                    <br>
                    <em></em>2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseNerftrinsic" role="button"
                           aria-expanded="false" aria-controls="collapseNerftrinsic">
                            Abstract
                        </a>
                        <a class="btn btn-dark"
                           href="https://arxiv.org/pdf/2303.09412.pdf">Arxiv</a>
                        <a class="btn btn-dark"
                           href="https://hannahhaensen.github.io/nerftrinsic_four/">GitHub</a>
                    </p>
                    <div class="collapse" id="collapseNerftrinsic">
                        <div class="card card-body">
                            <p>
                                We utilize Gaussian Fourier features to estimate extrinsic camera parameters and
                                dynamically predict varying intrinsic camera parameters through the supervision of the
                                projection
                                error. Our approach outperforms existing joint optimization methods on LLFF and BLEFF.
                                In
                                addition to these existing datasets, we introduce a new dataset called iFF with varying
                                intrinsic
                                camera parameters. NeRFtrinsic Four is a step forward in joint optimization NeRF-based
                                view synthesis and enables more realistic and flexible rendering in real-world scenarios
                                with
                                varying camera parameters.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="80%"
                         src="https://github-production-user-asset-6210df.s3.amazonaws.com/22636930/272610460-e1e407ef-4249-4e5e-bc38-486e08204548.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</span>
                    </a>
                    <br>
                    <span style="font-weight: lighter">N. Schischka*</span>, <span>H. Schieber*</span>, <span
                        style="font-weight: lighter">M. A. Karaoglu*, M. Görgülü*, F. Grötzner, A. Ladikos, D. Roth, N.
                    Navab and B. Busam,  *denotes equal contribution</span>
                    <br>
                    <em></em>2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseDynamon" role="button"
                           aria-expanded="false" aria-controls="collapseDynamon">
                            Abstract
                        </a>
                        <a class="btn btn-dark"
                           href="https://arxiv.org/pdf/2309.08927.pdf">Arxiv</a>
                        <a class="btn btn-dark"
                           href="https://hannahhaensen.github.io/DynaMoN/">GitHub/Website</a>
                    </p>
                    <div class="collapse" id="collapseDynamon">
                        <div class="card card-body">
                            <p>
                                Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera
                                poses. These are often hard to retrieve with existing structure-from-motion (SfM)
                                pipelines as
                                both camera and scene content can change. We propose DynaMoN that leverages simultaneous
                                localization and mapping (SLAM) jointly with motion masking to handle dynamic scene
                                content. Our robust SLAM-based tracking module significantly accelerates the training
                                process of
                                the dynamic NeRF while improving the quality of synthesized views at the same time.
                                Extensive experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's
                                iPhone
                                dataset, three real-world datasets, shows the advantages of DynaMoN both for camera pose
                                estimation and novel view synthesis.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="260"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/h6d.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">HouseCat6D--A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</span>
                    </a>
                    <br>
                    <span style="font-weight: lighter">H. Jung*, S.-C. Wu*, P. Ruhkamp*, </span>H. Schieber*<span
                        style="font-weight: lighter">, P. Wang, G. Rizzoli, H. Zhao, S. D. Meier, D. Roth,
                    N. Navab and B. Busam,  *denotes equal contribution</span>
                    <br>
                    <em></em>2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseHousecat6d" role="button"
                           aria-expanded="false" aria-controls="collapseHousecat6d">
                            Abstract
                        </a>
                        <a class="btn btn-dark"
                           href="https://arxiv.org/pdf/2212.10428.pdf">Arxiv</a>
                        <a class="btn btn-dark"
                           href="https://sites.google.com/view/housecat6d">Website</a>
                        <a class="btn btn-dark"
                           href="https://sites.google.com/view/housecat6d/dataset">Dataset</a>
                        <a class="btn btn-dark"
                           href="https://github.com/Junggy/HouseCat6D">Toolbox</a>
                    </p>
                    <div class="collapse" id="collapseHousecat6d">
                        <div class="card card-body">
                            <p>
                                Estimating 6D object poses is a major challenge in 3D computer vision. Building on
                                successful instance-level approaches, research is shifting towards category-level pose
                                estimation for practical applications. Current category-level datasets, however, fall
                                short in annotation quality and pose variety. Addressing this, we introduce HouseCat6D,
                                a new category-level 6D pose dataset. It features 1) multi-modality with Polarimetric
                                RGB and Depth (RGBD+P), 2) encompasses 194 diverse objects across 10 household
                                categories, including two photometrically challenging ones, and 3) provides high-quality
                                pose annotations with an error range of only 1.35 mm to 1.74 mm. The dataset also
                                includes 4) 41 large-scale scenes with comprehensive viewpoint and occlusion coverage,
                                5) a checkerboard-free environment, and 6) dense 6D parallel-jaw robotic grasp
                                annotations. Additionally, we present benchmark results for leading category-level pose
                                estimation networks.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <section>
        <br>
        Copyright @ Hannah Schieber | This is my personal webpage.
        <br>
        <br>
        <br>
    </section>
</div>

<script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.8/dist/umd/popper.min.js"
        integrity="sha384-I7E8VVD/ismYTF4hNIPjVp/Zjvgyol6VFvRkX/vR+Vc4jQkC+hVqc2pM8ODewa9r"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.min.js"
        integrity="sha384-0pUGZvbkm6XF6gxjEnlmuGrJXVbNuzT9qBBavbLwCsOGabYfZo0T0to5eqruptLy"
        crossorigin="anonymous"></script>


</body>
</html>
