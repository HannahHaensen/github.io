<!DOCTYPE HTML>
<html lang="en">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Hannah Schieber</title>

    <meta name="author" content="Hannah Schieber">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <meta content="Hannah Schieber, PhD Student" property="og:title">
    <meta content="Hannah Schieber, PhD Student" property="description">
    <meta content="Hannah Schieber, personal webpage" property="description">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet"
          integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">

</head>

<body>
<div class="container">
    <section>
        <br>
        <br>
        <br>
        <div class="container">
            <div class="row">
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <p class="name" style="text-align: center;">
                        Hannah Schieber, M. Sc.
                    </p>
                    <p>
                        I am a PhD student at the Department of Artificial Intelligence in Biomedical Engineering,
                        Friedrich-Alexander University Erlangen-Nürnberg (FAU). I am currently researching the topic
                        of smart remote extended reality teleconsultation and product guidance.
                    </p>
                    <p>
                        Moreover, I am an research assistant at Technical University of Munich, assisting as tutor
                        at several courses like:
                        <a href="https://www.cs.cit.tum.de/camp/news/article/workshop-in-advanced-topics-in-3d-computer-vision-july-2023/">Advanced
                            Topics in 3D Computer Vision</a>
                        or <a
                            href="https://www.cs.cit.tum.de/camp/teaching/seminars/modern-computer-vision-methods-ws-2023-24/">Modern
                        Computer Vision Methods</a>
                    </p>
                    <p>
                        Prior that, I received my bachelor’s degree from HS Aalen, and my master’s degree from TH
                        Ingolstadt, both in computer science. During my studies, I gained experience in the industry
                        at MAPAL Dr. Kress KG, c-Com GmbH, Carl Zeiss AG and AUDI AG.
                    </p>
                    <p>Interested in a research coperation? Feel free to contact me any time via e-mail:</p>
                    <p style="text-align:center">
                        hannah dot schieber @ tum dot de &nbsp;/&nbsp;
                        <a href="https://scholar.google.com/citations?user=1tKoj0EAAAAJ&hl=de">Google Scholar</a>
                        &nbsp;/&nbsp;
                        <a href="https://twitter.com/hannah_haensen">Twitter</a> &nbsp;/&nbsp;
                        <a href="https://github.com/HannahHaensen/">Github</a>
                    </p>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12">
                    <a href="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/hannah-rgb-240x320.jpg">
                        <img style="width:100%;max-width:100%" alt="profile photo"
                             src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/hannah-rgb-240x320.jpg"
                             class="hoverZoomLink"></a>
                </div>

            </div>
        </div>
    </section>
    <br>
    <section>
        <div class="container">
            <div class="row">
                <div class="col-12">
                    <h2>Research</h2>
                    <p>
                        I am interested in computer vision, extended reality and in general in many things in life.
                        Besides being passionate about my PhD studies I like to go cycling or go bouldering.
                    </p>
                    <p>
                        <b>Projects: </b>
                        Below you can find published research projects and at the bottom of the page some preprints.
                    </p>
                    <p>
                        <b>Services: </b><br>
                        <i>Conferences</i>
                        I was reviewer at IEEE VR, IEEE ISMAR and the WICV workshop at ICCV.
                        <i>Journals</i>
                        Springer IJCARS
                    </p>
                </div>
            </div>
        </div>
    </section>
    <section>
        <div class="container">
            <h4>2024</h4>
            <hr>

            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://hannahhaensen.github.io/mr-sense/images/system.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">Indoor Synthetic Data Generation: A Systematic Review</span>
                    </a>
                    <br>
                    H Schieber, C. Kleinbeck, L. Theelke, M. Kraft, J. Kreimeier and D. Roth
                    <br>
                    <em>IEEE AIxVR, </em>2024
                    <br>
                    <a href="https://hannahhaensen.github.io/mr-sense/">[GitHub/Website]</a>
                    <p>
                        Search tasks can be challenging for blind or visually impaired people. To determine an object's
                        location and to navigate there, they often rely on the limited sensory capabilities of a white
                        cane,
                        search haptically, or ask for help. We introduce MR-Sense, a mixed reality assistant to support
                        search and navigation tasks. The system is designed in a participatory fashion and utilizes
                        sensory
                        data of a standalone mixed reality head-mounted display to perform deep learning-driven object
                        recognition and environment mapping. The user is supported in object search tasks via spatially
                        mapped audio and vibrotactile feedback. We conducted a preliminary user study including ten
                        blind or
                        visually impaired participants and a final user evaluation with thirteen blind or visually
                        impaired
                        participants. The final study reveals that MR-Sense alone cannot replace the cane but provides a
                        valuable addition in terms of usability and task load. We further propose a standardized
                        evaluation
                        setup for replicable studies and highlight relevant potentials and challenges fostering future
                        work
                        towards employing technology in accessibility.
                    </p>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/timeline.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">Indoor Synthetic Data Generation: A Systematic Review</span>
                    </a>
                    <br>
                    H Schieber, K. C. Demir, C. Kleinbeck, S. H. Yang and D. Roth
                    <br>
                    <em>Computer Vision and Image Understanding (CVIU), </em>2024
                    <br>
                    <a
                            href="https://www.sciencedirect.com/science/article/pii/S1077314223002874">[Elsevier
                        CVIU]</a>
                    <a
                            href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4528507">[SSRN Preprint]</a>
                    <p>
                        Deep learning-based object recognition, 6D pose estimation, and semantic scene understanding
                        requires a large amount of training data to achieve generalization. Time-consuming annotation
                        processes, privacy, and security aspects lead to a scarcity of real-world datasets. To overcome
                        this lack of data, synthetic data generation has been proposed, including multiple facets in the
                        area of domain randomization to extend the data distribution. The objective of this review is to
                        identify methods applied for synthetic data generation aiming to improve 6D pose estimation,
                        object recognition, and semantic scene understanding in indoor scenarios. We further review
                        methods used to extend the data distribution and discuss best practices to bridge the gap
                        between synthetic and real-world data.
                    </p>
                </div>
            </div>

        </div>
    </section>
    <!-- 2023 -->
    <section>

        <div class="container">
            <h4>2023</h4>
            <hr>
            <div class="row">
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">A Modular Approach for 3D Reconstruction with Point Cloud Overlay (Poster)</span>

                    <br>
                    H. Schieber, F. Schmid, M. UI-Hassan, S. Zollmann and D. Roth
                    <br>
                    <em>Poster Session at ISMAR</em>, 2023
                    <br>
                    <a href="https://ieeexplore.ieee.org/document/10322162">[IEEE]</a>
                    <p>
                        We present a modular approach allowing the flexible exchange of the individual part, i.e.
                        the camera or SLAM algorithm. This work presents results from a pilot study involving five
                        participants to gain an impression of what kind of visualization type would be preferred and
                        whether the point cloud overlay would assist the user in recognizing changes in the
                        surroundings. The point cloud overlay enabled the participants to perceive more changes. The
                        pilot study revealed that 60% of the participants showed a preference for the point cloud
                        overlay over the pure mesh representation.
                    </p>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/F8pKEfDWsAA8TmL.jpg">
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <span class="papertitle">Injured Avatars: The Impact of Embodied Anatomies and Virtual Injuries on Well-being and Performance</span>

                    <br>
                    C. Kleinbeck, H. Schieber, J. Kreimeier, A. Martin-Gomez, M. Unberath and D. Roth
                    <br>
                    <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2023
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10269734">[IEEE TVCG]</a>
                    <p></p>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20145018.png">
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <span class="papertitle">Deep Learning in Surgical Workflow Analysis: A Review of Phase and Step Recognition</span>

                    <br>
                    Demir, K. C., Schieber, H., WeiseRoth, T., May, M., Maier, A., & Yang, S. H.
                    <br>
                    <em>IEEE Journal of Biomedical and Health Informatics</em>, 2023
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10238472">[IEEE]</a>
                    <p></p>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20144745.png">
                </div>
            </div>

        </div>


    </section>
    <!-- 2022 -->
    <section>
        <div class="container">
            <h4>2022</h4>
            <hr>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="50%"
                         src="https://user-images.githubusercontent.com/22636930/250826756-42a2da46-a9e9-41e6-83e8-c01f7bca38ba.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">ARTFM: Augmented Reality Visualization of Tool Functionality Manuals in Operating Rooms. (Poster)</span>
                    </a>
                    <br>
                    Kleinbeck, C., Schieber, H., Andress, S., Krautz, C., & Roth, D.
                    <br>
                    <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                        (pp. 736-737). IEEE.</em>
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9757491">[IEEE]</a>
                    <p>
                        Error-free surgical procedures are crucial for a patient's health. However, with the
                        increasing complexity and variety of surgical instruments, it is difficult for clinical
                        staff to acquire detailed assembly and usage knowledge leading to errors in process and
                        preparation steps. Yet, the gold standard in retrieving necessary information when problems
                        occur is to get the paperbased manual. Reading through the necessary instructions is
                        time-consuming and decreases care quality. We propose ARTFM, a process integrated manual,
                        highlighting the correct parts needed, their location, and step-by-step instructions to
                        combine the instrument using an augmented reality head-mounted display.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="50%"
                         src="https://user-images.githubusercontent.com/22636930/250825228-2340df9b-920a-4378-8fa1-956ba771fd79.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">A Mixed Reality Guidance System for Blind and Visually Impaired People (Poster)</span>

                    <br>
                    Schieber, H., Kleinbeck, C., Pradel, C., Theelke, L., & Roth, D.
                    <br>
                    <em>2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)
                        (pp. 726-727). IEEE.</em>, 2022
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9757681">[IEEE]</a>
                    <p></p>
                    <p>
                        Persons affected by blindness or visual impairments are challenged by spatially
                        understanding unfamiliar environments. To obtain such understanding, they have to sense
                        their environment closely and carefully. Especially objects outside the sensing area of
                        analog assistive devices, such as a white cane, are simply not perceived and can be the
                        cause of collisions. This project proposes a mixed reality guidance system that aims at
                        preventing such problems. We use object detection and the 3D sensing capabilities of a mixed
                        reality head mounted device to inform users about their spatial surroundings.
                    </p>
                </div>
            </div>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="50%"
                         src="https://user-images.githubusercontent.com/22636930/250823795-80184232-0e85-4b26-967a-fa62b517adb3.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">Deep Sensor Fusion with Pyramid Fusion Networks for 3D Semantic Segmentation</span>

                    <br>
                    H. Schieber, F. Duerr, T. Schoen and J. Beyerer
                    <br>
                    <em>Intelligent Vehicles Symposium (IV), Aachen, Germany</em>, 2022
                    <br>
                    <a href="https://ieeexplore.ieee.org/document/9827113">[IEEE]</a>

                    <a
                            href="https://arxiv.org/pdf/2205.13629.pdf">[Arxiv]</a>
                    <p></p>
                </div>
            </div>
        </div>

    </section>
    <!-- Preprints -->
    <section>
        <div class="container">
            <h4>Preprints</h4>
            <hr>

            <div class="row">
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">NeRFtrinsic Four: An End-To-End Trainable NeRF Jointly Optimizing Diverse Intrinsic and Extrinsic Camera Parameters</span>

                    <br>
                    H. Schieber, F. Deuser, B. Egger, N. Oswald and D. Roth
                    <br>
                    <em></em>2023
                    <br>
                    <a
                            href="https://arxiv.org/pdf/2303.09412.pdf">[Arxiv]</a>
                    <a
                            href="https://hannahhaensen.github.io/nerftrinsic_four/">[GitHub]</a>
                    <p>
                        We utilize Gaussian Fourier features to estimate extrinsic camera parameters and dynamically
                        predict varying intrinsic camera parameters through the supervision of the projection error.
                        Our approach outperforms existing joint optimization methods on LLFF and BLEFF. In addition
                        to these existing datasets, we introduce a new dataset called iFF with varying intrinsic
                        camera parameters. NeRFtrinsic Four is a step forward in joint optimization NeRF-based view
                        synthesis and enables more realistic and flexible rendering in real-world scenarios with
                        varying camera parameters.
                    </p>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="260"
                         src="https://user-images.githubusercontent.com/22636930/231704527-8c070d6b-0ac8-4432-9bd2-17725a04d191.png">
                </div>
            </div>
            <div class="row">
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">DynaMoN: Motion-Aware Fast And Robust Camera Localization for Dynamic NeRF</span>
                    </a>
                    <br>
                    M. A. Karaoglu, H. Schieber, N. Schischka, M. Görgülü, F. Grötzner, A. Ladikos, D. Roth, N.
                    Navab and B. Busam
                    <br>
                    <em></em>2023
                    <br>
                    <a
                            href="https://arxiv.org/pdf/2309.08927.pdf">[Arxiv]</a>
                    <a
                            href="https://hannahhaensen.github.io/DynaMoN/">[GitHub/Website]</a>
                    <p>
                        Dynamic reconstruction with neural radiance fields (NeRF) requires accurate camera poses.
                        These are often hard to retrieve with existing structure-from-motion (SfM) pipelines as both
                        camera and scene content can change. We propose DynaMoN that leverages simultaneous
                        localization and mapping (SLAM) jointly with motion masking to handle dynamic scene content.
                        Our robust SLAM-based tracking module significantly accelerates the training process of the
                        dynamic NeRF while improving the quality of synthesized views at the same time. Extensive
                        experimental validation on TUM RGB-D, BONN RGB-D Dynamic and the DyCheck's iPhone dataset,
                        three real-world datasets, shows the advantages of DynaMoN both for camera pose estimation
                        and novel view synthesis.
                    </p>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="260"
                         src="https://github-production-user-asset-6210df.s3.amazonaws.com/22636930/272610460-e1e407ef-4249-4e5e-bc38-486e08204548.png">
                </div>
            </div>
            <div class="row">

                <div class="col-lg-8 col-md-8 col-sm-12">
                    <a>
                        <span class="papertitle">HouseCat6D--A Large-Scale Multi-Modal Category Level 6D Object Pose Dataset with Household Objects in Realistic Scenarios</span>
                    </a>
                    <br>
                    H. Jung, S.-C. Wu, P. Ruhkamp, H. Schieber, P. Wang, G. Rizzoli, H. Zhao, S. D. Meier, D. Roth,
                    N. Navab and B. Busam
                    <br>
                    <em></em>2023
                    <br>
                    <a
                            href="https://arxiv.org/pdf/2212.10428.pdf">[Arxiv]</a>
                    <a
                            href="https://sites.google.com/view/housecat6d">[Website]</a>
                    <a
                            href="https://sites.google.com/view/housecat6d/dataset">[Dataset]</a>
                    <a
                            href="https://github.com/Junggy/HouseCat6D">[Toolbox]</a>
                    <p></p>
                </div>
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="260"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/h6d.png">
                </div>
            </div>
        </div>
    </section>
</div>


</body>
</html>
