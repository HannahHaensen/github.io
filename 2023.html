    <section>
        <br>
        <div class="container">
            <hr>
            <h4 id="2023">2023</h4>
            <hr>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/F8pKEfDWsAA8TmL.jpg">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">

                    <span class="papertitle">A Modular Approach for 3D Reconstruction with Point Cloud Overlay (Poster)</span>

                    <br>
                    <span>H. Schieber</span>., <span style="font-weight: lighter">F. Schmid, M. UI-Hassan, S. Zollmann and D. Roth</span>
                    <br>
                    <em>Poster Session at ISMAR</em>, 2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn  btn-dark" data-bs-toggle="collapse" href="#collapseISMAR3D" role="button"
                           aria-expanded="false" aria-controls="collapseISMAR3D">
                            Abstract
                        </a>
                        <a class="btn  btn-dark" href="https://ieeexplore.ieee.org/document/10322162">IEEE</a>
                    </p>
                    <div class="collapse" id="collapseISMAR3D">
                        <div class="card card-body">
                            <p>
                                We present a modular approach allowing the flexible exchange of the individual part,
                                i.e.
                                the camera or SLAM algorithm. This work presents results from a pilot study involving
                                five
                                participants to gain an impression of what kind of visualization type would be preferred
                                and
                                whether the point cloud overlay would assist the user in recognizing changes in the
                                surroundings. The point cloud overlay enabled the participants to perceive more changes.
                                The
                                pilot study revealed that 60% of the participants showed a preference for the point
                                cloud
                                overlay over the pure mesh representation.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20145018.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <span class="papertitle">Injured Avatars: The Impact of Embodied Anatomies and Virtual Injuries on Well-being and Performance</span>

                    <br>
                    <span style="font-weight: lighter">C. Kleinbeck, </span>H. Schieber<span
                        style="font-weight: lighter">, J. Kreimeier, A. Martin-Gomez, M. Unberath and D. Roth
                </span><br>
                    <em>IEEE Transactions on Visualization and Computer Graphics</em>, 2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseInjury" role="button"
                           aria-expanded="false" aria-controls="collapseInjury">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/abstract/document/10269734">IEEE
                            TVCG</a>
                    </p>
                    <div class="collapse" id="collapseInjury">
                        <div class="card card-body">
                            <p>
                                Human cognition relies on embodiment as a fundamental mechanism. Virtual avatars allow
                                users to experience the adaptation, control, and perceptual illusion of alternative
                                bodies. Although virtual bodies have medical applications in motor rehabilitation and
                                therapeutic interventions, their potential for learning anatomy and medical
                                communication remains underexplored. For learners and patients, anatomy, procedures, and
                                medical imaging can be abstract and difficult to grasp. Experiencing anatomies,
                                injuries, and treatments virtually through one's own body could be a valuable tool for
                                fostering understanding. This work investigates the impact of avatars displaying anatomy
                                and injuries suitable for such medical simulations. We ran a user study utilizing a
                                skeleton avatar and virtual injuries, comparing to a healthy human avatar as a baseline.
                                We evaluate the influence on embodiment, well-being, and presence with self-report
                                questionnaires, as well as motor performance via an arm movement task. Our results show
                                that while both anatomical representation and injuries increase feelings of eeriness,
                                there are no negative effects on embodiment, well-being, presence, or motor performance.
                                These findings suggest that virtual representations of anatomy and injuries are suitable
                                for medical visualizations targeting learning or communication without significantly
                                affecting users' mental state or physical control within the simulation.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
            <br>
            <div class="row">
                <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                    <img width="100%"
                         src="https://raw.githubusercontent.com/HannahHaensen/hannahhaensen.github.io/gh-pages/assets/img/Screenshot%202023-10-16%20144745.png">
                </div>
                <div class="col-lg-8 col-md-8 col-sm-12">
                    <span class="papertitle">Deep Learning in Surgical Workflow Analysis: A Review of Phase and Step Recognition</span>

                    <br>
                    <span style="font-weight: lighter">Demir, K. C.,</span> Schieber, H., <span
                        style="font-weight: lighter">Weise, T. Roth, D., May, M., Maier, A., & Yang, S. H.
                </span><br>
                    <em>IEEE Journal of Biomedical and Health Informatics</em>, 2023
                    <br>
                    <p class="d-inline-flex gap-1">
                        <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseRev" role="button"
                           aria-expanded="false" aria-controls="collapseRev">
                            Abstract
                        </a>
                        <a class="btn btn-dark" href="https://ieeexplore.ieee.org/abstract/document/10238472">IEEE
                            JBHI</a>
                    </p>
                    <div class="collapse" id="collapseRev">
                        <div class="card card-body">
                            <p>
                                Objective: In the last two decades, there has been a growing interest in exploring
                                surgical procedures with statistical models to analyze operations at different semantic
                                levels. This information is necessary for developing context-aware intelligent systems,
                                which can assist the physicians during operations, evaluate procedures afterward or help
                                the management team to effectively utilize the operating room. The objective is to
                                extract reliable patterns from surgical data for the robust estimation of surgical
                                activities performed during operations. The purpose of this article is to review the
                                state-of-the-art deep learning methods that have been published after 2018 for analyzing
                                surgical workflows, with a focus on phase and step recognition. Methods: Three
                                databases, IEEE Xplore, Scopus, and PubMed were searched, and additional studies are
                                added through a manual search. After the database search, 343 studies were screened and
                                a total of 44 studies are selected for this review. Conclusion: The use of temporal
                                information is essential for identifying the next surgical action. Contemporary methods
                                used mainly RNNs, hierarchical CNNs, and Transformers to preserve long-distance temporal
                                relations. The lack of large publicly available datasets for various procedures is a
                                great challenge for the development of new and robust models. As supervised learning
                                strategies are used to show proof-of-concept, self-supervised, semi-supervised, or
                                active learning methods are used to mitigate dependency on annotated data. Significance:
                                The present study provides a comprehensive review of recent methods in surgical workflow
                                analysis, summarizes commonly used architectures, datasets, and discusses challenges.
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>